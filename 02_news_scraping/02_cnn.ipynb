{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "186baa0a",
   "metadata": {},
   "source": [
    "# Scrap Latest News from CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115f8a49",
   "metadata": {},
   "source": [
    "## Headlines and Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94f8f58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CNN News Headlines:\n",
      "1. Brooklyn Bridge incident\n",
      "   https://www.cnn.com/2025/05/17/us/manhattan-brooklyn-bridge-ship\n",
      "2. Brooklyn Bridge incident\n",
      "   https://www.cnn.com/2025/05/17/us/manhattan-brooklyn-bridge-ship\n",
      "3. Russia-Ukraine talks\n",
      "   https://www.cnn.com/2025/05/16/europe/ukraine-russia-talks-npw-analysis-intl\n",
      "4. Russia-Ukraine talks\n",
      "   https://www.cnn.com/2025/05/16/europe/ukraine-russia-talks-npw-analysis-intl\n",
      "5. Romania’s presidential election re-run\n",
      "   https://www.cnn.com/2025/05/16/europe/romania-presidential-election-simion-dan-intl-cmd\n",
      "6. Romania’s presidential election re-run\n",
      "   https://www.cnn.com/2025/05/16/europe/romania-presidential-election-simion-dan-intl-cmd\n",
      "7. Eurovision Song Contest\n",
      "   https://www.cnn.com/2025/05/17/entertainment/eurovision-song-contest-winner-austria-jj-latam-intl\n",
      "8. Eurovision Song Contest\n",
      "   https://www.cnn.com/2025/05/17/entertainment/eurovision-song-contest-winner-austria-jj-latam-intl\n",
      "9. Poland election\n",
      "   https://www.cnn.com/2025/05/16/europe/poland-presidential-election-tusk-intl\n",
      "10. Poland election\n",
      "   https://www.cnn.com/2025/05/16/europe/poland-presidential-election-tusk-intl\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "\n",
    "def get_cnn_news():\n",
    "    \"\"\"Extract headlines from CNN's website using more reliable methods\"\"\"\n",
    "    url = \"https://www.cnn.com\"  # Use main CNN domain which has more content\n",
    "    \n",
    "    try:\n",
    "        # Make request with headers and appropriate timeout\n",
    "        response = requests.get(url, headers=HEADERS, timeout=15)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: Received status code {response.status_code}\")\n",
    "            return []\n",
    "            \n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # CNN structure changes frequently - try multiple strategies\n",
    "        headlines = []\n",
    "        seen_urls = set()\n",
    "        \n",
    "        # Strategy 1: Find headline containers directly\n",
    "        for container in soup.select('div[class*=\"headline\"], span[class*=\"headline\"], h3[class*=\"headline\"]'):\n",
    "            link_tag = container.find('a', href=True) or container.find_parent('a', href=True)\n",
    "            if link_tag:\n",
    "                text = container.get_text(strip=True)\n",
    "                link = link_tag.get('href', '')\n",
    "                if text and link and link not in seen_urls:\n",
    "                    if link.startswith('/'):\n",
    "                        link = f\"https://www.cnn.com{link}\"\n",
    "                    headlines.append((text, link))\n",
    "                    seen_urls.add(link)\n",
    "        \n",
    "        # Strategy 2: Find all article links by their URL pattern\n",
    "        if len(headlines) < 5:\n",
    "            for link_tag in soup.find_all('a', href=True):\n",
    "                link = link_tag.get('href', '')\n",
    "                text = link_tag.get_text(strip=True)\n",
    "                \n",
    "                # CNN articles typically include year in URL\n",
    "                if (text and link and link not in seen_urls and \n",
    "                    ('/20' in link or 'article' in link) and \n",
    "                    not link.endswith('.jpg') and not link.endswith('.png')):\n",
    "                    \n",
    "                    if link.startswith('/'):\n",
    "                        link = f\"https://www.cnn.com{link}\"\n",
    "                    \n",
    "                    # Ensure it's a CNN link\n",
    "                    if 'cnn.com' in link:\n",
    "                        headlines.append((text, link))\n",
    "                        seen_urls.add(link)\n",
    "        \n",
    "        # Limit to 10 headlines\n",
    "        return headlines[:10]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping CNN: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"\\nCNN News Headlines:\")\n",
    "for idx, (text, link) in enumerate(get_cnn_news(), 1):\n",
    "    print(f\"{idx}. {text}\\n   {link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dd7f3d",
   "metadata": {},
   "source": [
    "## Save to file with Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47029956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN News Headlines with Content and Images:\n",
      "Fetching article 1/5: Brooklyn Bridge incident\n",
      "  Saved image: _images_cnn_20250518\\16f2d32bf2\\image_1.jpg\n",
      "  Saved image: _images_cnn_20250518\\16f2d32bf2\\image_2.jpg\n",
      "  Saved image: _images_cnn_20250518\\16f2d32bf2\\image_3.jpg\n",
      "Fetching article 2/5: Brooklyn Bridge incident\n",
      "  Saved image: _images_cnn_20250518\\16f2d32bf2\\image_1.jpg\n",
      "  Saved image: _images_cnn_20250518\\16f2d32bf2\\image_2.jpg\n",
      "  Saved image: _images_cnn_20250518\\16f2d32bf2\\image_3.jpg\n",
      "Fetching article 3/5: Russia-Ukraine talks\n",
      "  Saved image: _images_cnn_20250518\\6c9d82cac4\\image_1.jpg\n",
      "  Saved image: _images_cnn_20250518\\6c9d82cac4\\image_2.JPG\n",
      "  Saved image: _images_cnn_20250518\\6c9d82cac4\\image_3.jpg\n",
      "Fetching article 4/5: Russia-Ukraine talks\n",
      "  Saved image: _images_cnn_20250518\\6c9d82cac4\\image_1.jpg\n",
      "  Saved image: _images_cnn_20250518\\6c9d82cac4\\image_2.JPG\n",
      "  Saved image: _images_cnn_20250518\\6c9d82cac4\\image_3.jpg\n",
      "Fetching article 5/5: Romania’s presidential election re-run\n",
      "  Saved image: _images_cnn_20250518\\e610f21c19\\image_1.jpg\n",
      "  Saved image: _images_cnn_20250518\\e610f21c19\\image_2.jpg\n",
      "\n",
      "1. Brooklyn Bridge incident\n",
      "   Link: https://www.cnn.com/2025/05/17/us/manhattan-brooklyn-bridge-ship\n",
      "   Content preview: At least two people were killed and more than a dozen others injured when a Mexican Navy training ship on a goodwill tour struck the underside of the Brooklyn Bridge in New York Saturday night, offici...\n",
      "   Downloaded 3 images\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. Brooklyn Bridge incident\n",
      "   Link: https://www.cnn.com/2025/05/17/us/manhattan-brooklyn-bridge-ship\n",
      "   Content preview: At least two people were killed and more than a dozen others injured when a Mexican Navy training ship on a goodwill tour struck the underside of the Brooklyn Bridge in New York Saturday night, offici...\n",
      "   Downloaded 3 images\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. Russia-Ukraine talks\n",
      "   Link: https://www.cnn.com/2025/05/16/europe/ukraine-russia-talks-npw-analysis-intl\n",
      "   Content preview: Much has happened this week, but what failed to transpire is the more telling.  The firstdirect talks between Ukraine and Russiashould have heralded a new era of diplomacy towards solving Europe’s lar...\n",
      "   Downloaded 3 images\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "4. Russia-Ukraine talks\n",
      "   Link: https://www.cnn.com/2025/05/16/europe/ukraine-russia-talks-npw-analysis-intl\n",
      "   Content preview: Much has happened this week, but what failed to transpire is the more telling.  The firstdirect talks between Ukraine and Russiashould have heralded a new era of diplomacy towards solving Europe’s lar...\n",
      "   Downloaded 3 images\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "5. Romania’s presidential election re-run\n",
      "   Link: https://www.cnn.com/2025/05/16/europe/romania-presidential-election-simion-dan-intl-cmd\n",
      "   Content preview: A hard-right nationalist is favored to win Romania’s presidential election run-off on Sunday – a vote being held five months after theoriginal election was annulled.  George Simion won 41% of ballots ...\n",
      "   Downloaded 2 images\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Saved all articles to cnn_news_20250518_115303.md\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "import hashlib\n",
    "\n",
    "# Define headers to avoid being blocked\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "}\n",
    "\n",
    "def get_cnn_news(save_images=True, fetch_content=True, max_articles=10):\n",
    "    \"\"\"\n",
    "    Extract headlines from CNN's website with enhanced functionality:\n",
    "    - Extracts headlines and links\n",
    "    - Fetches article content (paragraphs)\n",
    "    - Downloads and saves images\n",
    "    - Creates organized folders\n",
    "    \n",
    "    Args:\n",
    "        save_images (bool): Whether to download and save images\n",
    "        fetch_content (bool): Whether to fetch the full article content\n",
    "        max_articles (int): Maximum number of articles to scrape\n",
    "        \n",
    "    Returns:\n",
    "        list: List of dictionaries containing article data\n",
    "    \"\"\"\n",
    "    url = \"https://www.cnn.com\"\n",
    "    \n",
    "    # Create directory for saving data\n",
    "    today = datetime.now().strftime(\"%Y%m%d\")\n",
    "    base_dir = f\"_images_cnn_{today}\"\n",
    "    if save_images and not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "    \n",
    "    try:\n",
    "        # Make request with headers and appropriate timeout\n",
    "        response = requests.get(url, headers=HEADERS, timeout=15)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: Received status code {response.status_code}\")\n",
    "            return []\n",
    "            \n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # Find headline containers using multiple strategies\n",
    "        headlines = []\n",
    "        seen_urls = set()\n",
    "        \n",
    "        # Strategy 1: Find headline containers directly\n",
    "        for container in soup.select('div[class*=\"headline\"], span[class*=\"headline\"], h3[class*=\"headline\"]'):\n",
    "            link_tag = container.find('a', href=True) or container.find_parent('a', href=True)\n",
    "            if link_tag:\n",
    "                text = container.get_text(strip=True)\n",
    "                link = link_tag.get('href', '')\n",
    "                if text and link and link not in seen_urls:\n",
    "                    if link.startswith('/'):\n",
    "                        link = f\"https://www.cnn.com{link}\"\n",
    "                    \n",
    "                    # Initialize article data dictionary\n",
    "                    article_data = {\n",
    "                        \"headline\": text,\n",
    "                        \"link\": link,\n",
    "                        \"content\": \"\",\n",
    "                        \"images\": []\n",
    "                    }\n",
    "                    \n",
    "                    headlines.append(article_data)\n",
    "                    seen_urls.add(link)\n",
    "        \n",
    "        # Strategy 2: Find all article links by their URL pattern\n",
    "        if len(headlines) < 3:\n",
    "            for link_tag in soup.find_all('a', href=True):\n",
    "                link = link_tag.get('href', '')\n",
    "                text = link_tag.get_text(strip=True)\n",
    "                \n",
    "                # CNN articles typically include year in URL\n",
    "                if (text and link and link not in seen_urls and \n",
    "                    ('/20' in link or 'article' in link) and \n",
    "                    not link.endswith('.jpg') and not link.endswith('.png')):\n",
    "                    \n",
    "                    if link.startswith('/'):\n",
    "                        link = f\"https://www.cnn.com{link}\"\n",
    "                    \n",
    "                    # Ensure it's a CNN link\n",
    "                    if 'cnn.com' in link:\n",
    "                        article_data = {\n",
    "                            \"headline\": text,\n",
    "                            \"link\": link,\n",
    "                            \"content\": \"\",\n",
    "                            \"images\": []\n",
    "                        }\n",
    "                        \n",
    "                        headlines.append(article_data)\n",
    "                        seen_urls.add(link)\n",
    "        \n",
    "        # Limit to max_articles\n",
    "        headlines = headlines[:max_articles]\n",
    "        \n",
    "        # Fetch article content and images if requested\n",
    "        if fetch_content:\n",
    "            for idx, article in enumerate(headlines):\n",
    "                print(f\"Fetching article {idx+1}/{len(headlines)}: {article['headline']}\")\n",
    "                \n",
    "                try:\n",
    "                    # Add a delay to avoid overwhelming the server\n",
    "                    time.sleep(1)\n",
    "                    \n",
    "                    # Fetch article page\n",
    "                    article_response = requests.get(article['link'], headers=HEADERS, timeout=15)\n",
    "                    if article_response.status_code != 200:\n",
    "                        print(f\"  Error: Could not fetch article. Status code: {article_response.status_code}\")\n",
    "                        continue\n",
    "                        \n",
    "                    article_soup = BeautifulSoup(article_response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Extract article content (paragraphs)\n",
    "                    # Look for the main content area with multiple fallbacks\n",
    "                    content_container = article_soup.select_one('div[class*=\"article__content\"], div[class*=\"body\"], div[class*=\"story-body\"]')\n",
    "                    if not content_container:\n",
    "                        content_container = article_soup  # Fallback to the entire page\n",
    "                    \n",
    "                    # Extract paragraphs\n",
    "                    paragraphs = content_container.find_all('p')\n",
    "                    article_text = '\\n\\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n",
    "                    article['content'] = article_text\n",
    "                    \n",
    "                    # Extract images if requested\n",
    "                    if save_images:\n",
    "                        # Find all images in the article\n",
    "                        images = content_container.find_all('img', src=True)\n",
    "                        \n",
    "                        # Create article-specific directory (using a hash of the headline to avoid invalid filenames)\n",
    "                        article_hash = hashlib.md5(article['headline'].encode()).hexdigest()[:10]\n",
    "                        article_dir = os.path.join(base_dir, article_hash)\n",
    "                        if not os.path.exists(article_dir):\n",
    "                            os.makedirs(article_dir)\n",
    "                        \n",
    "                        # Download and save images\n",
    "                        for i, img in enumerate(images):\n",
    "                            img_url = img['src']\n",
    "                            \n",
    "                            # Skip data URLs and invalid URLs\n",
    "                            if img_url.startswith('data:') or not img_url or img_url == '#':\n",
    "                                continue\n",
    "                                \n",
    "                            # Make URL absolute if needed\n",
    "                            if img_url.startswith('/'):\n",
    "                                img_url = f\"https://www.cnn.com{img_url}\"\n",
    "                                \n",
    "                            try:\n",
    "                                # Get the image extension\n",
    "                                img_ext = os.path.splitext(urlparse(img_url).path)[1]\n",
    "                                if not img_ext:\n",
    "                                    img_ext = '.jpg'  # Default extension\n",
    "                                \n",
    "                                # Create the image filename\n",
    "                                img_filename = f\"image_{i+1}{img_ext}\"\n",
    "                                img_path = os.path.join(article_dir, img_filename)\n",
    "                                \n",
    "                                # Download the image\n",
    "                                img_response = requests.get(img_url, headers=HEADERS, timeout=10)\n",
    "                                if img_response.status_code == 200:\n",
    "                                    with open(img_path, 'wb') as f:\n",
    "                                        f.write(img_response.content)\n",
    "                                    article['images'].append({\n",
    "                                        'url': img_url,\n",
    "                                        'local_path': img_path\n",
    "                                    })\n",
    "                                    print(f\"  Saved image: {img_path}\")\n",
    "                            except Exception as e:\n",
    "                                print(f\"  Error saving image {img_url}: {e}\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"  Error processing article: {e}\")\n",
    "        \n",
    "        return headlines\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping CNN: {e}\")\n",
    "        return []\n",
    "\n",
    "# Get the news with enhanced functionality\n",
    "print(\"CNN News Headlines with Content and Images:\")\n",
    "articles = get_cnn_news(save_images=True, fetch_content=True, max_articles=5)\n",
    "\n",
    "# Display the results\n",
    "for idx, article in enumerate(articles, 1):\n",
    "    print(f\"\\n{idx}. {article['headline']}\")\n",
    "    print(f\"   Link: {article['link']}\")\n",
    "    \n",
    "    # Show content preview (first 200 characters)\n",
    "    content_preview = article['content'][:200].replace('\\n', ' ')\n",
    "    if article['content'] and len(article['content']) > 200:\n",
    "        content_preview += \"...\"\n",
    "    print(f\"   Content preview: {content_preview}\")\n",
    "    \n",
    "    # Show images\n",
    "    if article['images']:\n",
    "        print(f\"   Downloaded {len(article['images'])} images\")\n",
    "    else:\n",
    "        print(\"   No images downloaded\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Save all data to a markdown file for easy viewing\n",
    "markdown_path = f\"cnn_news_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "with open(markdown_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"# CNN News Articles - {datetime.now().strftime('%Y-%m-%d')}\\n\\n\")\n",
    "    \n",
    "    for idx, article in enumerate(articles, 1):\n",
    "        f.write(f\"## {idx}. {article['headline']}\\n\\n\")\n",
    "        f.write(f\"**Link**: [{article['link']}]({article['link']})\\n\\n\")\n",
    "        \n",
    "        if article['content']:\n",
    "            f.write(\"### Content:\\n\\n\")\n",
    "            f.write(article['content'].replace('\\n', '\\n\\n'))\n",
    "            f.write(\"\\n\\n\")\n",
    "        \n",
    "        if article['images']:\n",
    "            f.write(\"### Images:\\n\\n\")\n",
    "            for img in article['images']:\n",
    "                f.write(f\"- [{img['url']}]({img['local_path']})\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"---\\n\\n\")\n",
    "\n",
    "print(f\"\\nSaved all articles to {markdown_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31a250c",
   "metadata": {},
   "source": [
    "## Avoid yourself from being in trouble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea41fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN News Headlines with Content and Images:\n",
      "\n",
      "1. Nervous about the prospect of empty shelves and inflation, the US president sent in his even-keeled, professional negotiators to Geneva\n",
      "   Link: https://www.cnn.com/2025/05/12/business/china-trade-deal-trump\n",
      "   Content preview: President Donald Trump’s shock-and-awe tariff approach threatened to rupture the global financial system and drive the US economy into recession. Nervous about the prospect of empty store shelves and ...\n",
      "   No images downloaded\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. Nervous about the prospect of empty shelves and inflation, the US president sent in his even-keeled, professional negotiators to Geneva\n",
      "   Link: https://www.cnn.com/2025/05/12/business/china-trade-deal-trump\n",
      "   Content preview: President Donald Trump’s shock-and-awe tariff approach threatened to rupture the global financial system and drive the US economy into recession. Nervous about the prospect of empty store shelves and ...\n",
      "   No images downloaded\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. Live UpdatesTrump, optimistic on truce with China, says tariffs won’t go back to 145%\n",
      "   Link: https://www.cnn.com/politics/live-news/us-china-tariffs-trade-talks-trump-05-12-25\n",
      "   Content preview: Live Updates  •Trade war breakthrough:The US and China agreed todrastically roll back tariffson each other’s goods for an initial 90 days,de-escalating a punishing trade warandbuoying global markets. ...\n",
      "   Downloaded 19 images\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "4. Trump, optimistic on truce with China, says tariffs won’t go back to 145%\n",
      "   Link: https://www.cnn.com/politics/live-news/us-china-tariffs-trade-talks-trump-05-12-25\n",
      "   Content preview: \n",
      "   No images downloaded\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "5. Dow soars after Trump team and China announce tariff truce\n",
      "   Link: https://www.cnn.com/2025/05/12/investing/stock-market-dow-trade-deal-china\n",
      "   Content preview: \n",
      "   No images downloaded\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Saved all articles to cnn_news_20250512_224931.md\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "import hashlib\n",
    "import random\n",
    "import logging\n",
    "from requests.exceptions import RequestException, Timeout, ConnectionError\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename='cnn_scraper.log'\n",
    ")\n",
    "logger = logging.getLogger('cnn_scraper')\n",
    "\n",
    "# Generate random user agent\n",
    "try:\n",
    "    ua = UserAgent()\n",
    "    random_ua = ua.random\n",
    "except:\n",
    "    random_ua = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "\n",
    "# Define headers to avoid being blocked\n",
    "HEADERS = {\n",
    "    'User-Agent': random_ua,\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml',\n",
    "    'Referer': 'https://www.google.com/',\n",
    "    'DNT': '1'\n",
    "}\n",
    "\n",
    "# List of free proxies - update these with working proxies\n",
    "PROXIES = [\n",
    "    'http://103.83.232.122:80',\n",
    "    'http://103.216.82.19:6666',\n",
    "    'http://34.87.84.105:80',\n",
    "    None  # No proxy as fallback\n",
    "]\n",
    "\n",
    "def get_cnn_news(save_images=True, fetch_content=True, max_articles=10, retry_limit=3):\n",
    "    \"\"\"\n",
    "    Extract headlines from CNN's website with enhanced functionality:\n",
    "    - Extracts headlines and links\n",
    "    - Fetches article content (paragraphs)\n",
    "    - Downloads and saves images\n",
    "    - Creates organized folders\n",
    "    - Uses proxies and implements retry logic\n",
    "    \n",
    "    Args:\n",
    "        save_images (bool): Whether to download and save images\n",
    "        fetch_content (bool): Whether to fetch the full article content\n",
    "        max_articles (int): Maximum number of articles to scrape\n",
    "        retry_limit (int): Max number of retries for failed requests\n",
    "        \n",
    "    Returns:\n",
    "        list: List of dictionaries containing article data\n",
    "    \"\"\"\n",
    "    url = \"https://www.cnn.com\"\n",
    "    \n",
    "    # Create directory for saving data\n",
    "    today = datetime.now().strftime(\"%Y%m%d\")\n",
    "    base_dir = f\"_images_cnn_{today}\"\n",
    "    if save_images and not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "    \n",
    "    try:\n",
    "        # Make request with headers, proxies and appropriate timeout\n",
    "        articles_fetched = False\n",
    "        for attempt in range(retry_limit):\n",
    "            try:\n",
    "                # Random delay to avoid scraping patterns\n",
    "                time.sleep(random.uniform(1, 3))\n",
    "                \n",
    "                # Try with different proxies\n",
    "                proxy = random.choice(PROXIES)\n",
    "                proxies = {'http': proxy, 'https': proxy} if proxy else None\n",
    "                \n",
    "                logger.info(f\"Connecting to CNN with proxy: {proxy}\")\n",
    "                response = requests.get(\n",
    "                    url, \n",
    "                    headers=HEADERS, \n",
    "                    proxies=proxies,\n",
    "                    timeout=15,\n",
    "                    verify=True\n",
    "                )\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    articles_fetched = True\n",
    "                    break\n",
    "                else:\n",
    "                    logger.warning(f\"Attempt {attempt+1}: Received status code {response.status_code}\")\n",
    "            \n",
    "            except (ConnectionError, Timeout) as e:\n",
    "                logger.warning(f\"Attempt {attempt+1} failed: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not articles_fetched:\n",
    "            logger.error(\"Failed to connect to CNN after multiple attempts\")\n",
    "            return []\n",
    "            \n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # Find headline containers using multiple strategies\n",
    "        headlines = []\n",
    "        seen_urls = set()\n",
    "        \n",
    "        # Strategy 1: Find headline containers directly\n",
    "        for container in soup.select('div[class*=\"headline\"], span[class*=\"headline\"], h3[class*=\"headline\"]'):\n",
    "            link_tag = container.find('a', href=True) or container.find_parent('a', href=True)\n",
    "            if link_tag:\n",
    "                text = container.get_text(strip=True)\n",
    "                link = link_tag.get('href', '')\n",
    "                if text and link and link not in seen_urls:\n",
    "                    if link.startswith('/'):\n",
    "                        link = f\"https://www.cnn.com{link}\"\n",
    "                    \n",
    "                    # Initialize article data dictionary\n",
    "                    article_data = {\n",
    "                        \"headline\": text,\n",
    "                        \"link\": link,\n",
    "                        \"content\": \"\",\n",
    "                        \"images\": [],\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "                    headlines.append(article_data)\n",
    "                    seen_urls.add(link)\n",
    "        \n",
    "        # Strategy 2: Find all article links by their URL pattern\n",
    "        if len(headlines) < 3:\n",
    "            for link_tag in soup.find_all('a', href=True):\n",
    "                link = link_tag.get('href', '')\n",
    "                text = link_tag.get_text(strip=True)\n",
    "                \n",
    "                # CNN articles typically include year in URL\n",
    "                if (text and link and link not in seen_urls and \n",
    "                    ('/20' in link or 'article' in link) and \n",
    "                    not link.endswith('.jpg') and not link.endswith('.png')):\n",
    "                    \n",
    "                    if link.startswith('/'):\n",
    "                        link = f\"https://www.cnn.com{link}\"\n",
    "                    \n",
    "                    # Ensure it's a CNN link\n",
    "                    if 'cnn.com' in link:\n",
    "                        article_data = {\n",
    "                            \"headline\": text,\n",
    "                            \"link\": link,\n",
    "                            \"content\": \"\",\n",
    "                            \"images\": [],\n",
    "                            \"timestamp\": datetime.now().isoformat()\n",
    "                        }\n",
    "                        \n",
    "                        headlines.append(article_data)\n",
    "                        seen_urls.add(link)\n",
    "        \n",
    "        # Limit to max_articles\n",
    "        headlines = headlines[:max_articles]\n",
    "        \n",
    "        # Fetch article content and images if requested\n",
    "        if fetch_content:\n",
    "            for idx, article in enumerate(headlines):\n",
    "                logger.info(f\"Fetching article {idx+1}/{len(headlines)}: {article['headline']}\")\n",
    "                \n",
    "                try:\n",
    "                    # Random delay between requests (1-5 seconds)\n",
    "                    time.sleep(random.uniform(1, 5))\n",
    "                    \n",
    "                    # Try multiple proxies for each article\n",
    "                    article_fetched = False\n",
    "                    for attempt in range(retry_limit):\n",
    "                        try:\n",
    "                            # Rotate proxies\n",
    "                            proxy = random.choice(PROXIES)\n",
    "                            proxies = {'http': proxy, 'https': proxy} if proxy else None\n",
    "                            \n",
    "                            # Fetch article page\n",
    "                            article_response = requests.get(\n",
    "                                article['link'], \n",
    "                                headers=HEADERS, \n",
    "                                proxies=proxies,\n",
    "                                timeout=15\n",
    "                            )\n",
    "                            \n",
    "                            if article_response.status_code == 200:\n",
    "                                article_fetched = True\n",
    "                                break\n",
    "                            else:\n",
    "                                logger.warning(f\"  Attempt {attempt+1}: Status code: {article_response.status_code}\")\n",
    "                        \n",
    "                        except (ConnectionError, Timeout) as e:\n",
    "                            logger.warning(f\"  Article fetch attempt {attempt+1} failed: {str(e)}\")\n",
    "                            continue\n",
    "                    \n",
    "                    if not article_fetched:\n",
    "                        logger.error(f\"  Failed to fetch article after {retry_limit} attempts\")\n",
    "                        continue\n",
    "                        \n",
    "                    article_soup = BeautifulSoup(article_response.content, \"html.parser\")\n",
    "                    \n",
    "                    # Extract article content (paragraphs)\n",
    "                    # Look for the main content area with multiple fallbacks\n",
    "                    content_container = article_soup.select_one('div[class*=\"article__content\"], div[class*=\"body\"], div[class*=\"story-body\"]')\n",
    "                    if not content_container:\n",
    "                        content_container = article_soup  # Fallback to the entire page\n",
    "                    \n",
    "                    # Extract paragraphs\n",
    "                    paragraphs = content_container.find_all('p')\n",
    "                    article_text = '\\n\\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n",
    "                    article['content'] = article_text\n",
    "                    \n",
    "                    # Extract images if requested\n",
    "                    if save_images:\n",
    "                        # Find all images in the article\n",
    "                        images = content_container.find_all('img', src=True)\n",
    "                        \n",
    "                        # Create article-specific directory (using a hash of the headline to avoid invalid filenames)\n",
    "                        article_hash = hashlib.md5(article['headline'].encode()).hexdigest()[:10]\n",
    "                        article_dir = os.path.join(base_dir, article_hash)\n",
    "                        if not os.path.exists(article_dir):\n",
    "                            os.makedirs(article_dir)\n",
    "                        \n",
    "                        # Download and save images\n",
    "                        for i, img in enumerate(images):\n",
    "                            img_url = img['src']\n",
    "                            \n",
    "                            # Skip data URLs and invalid URLs\n",
    "                            if img_url.startswith('data:') or not img_url or img_url == '#':\n",
    "                                continue\n",
    "                                \n",
    "                            # Make URL absolute if needed\n",
    "                            if img_url.startswith('/'):\n",
    "                                img_url = f\"https://www.cnn.com{img_url}\"\n",
    "                                \n",
    "                            try:\n",
    "                                # Random delay between image downloads\n",
    "                                time.sleep(random.uniform(0.5, 2))\n",
    "                                \n",
    "                                # Get the image extension\n",
    "                                img_ext = os.path.splitext(urlparse(img_url).path)[1]\n",
    "                                if not img_ext:\n",
    "                                    img_ext = '.jpg'  # Default extension\n",
    "                                \n",
    "                                # Create the image filename\n",
    "                                img_filename = f\"image_{i+1}{img_ext}\"\n",
    "                                img_path = os.path.join(article_dir, img_filename)\n",
    "                                \n",
    "                                # Try multiple proxies for each image\n",
    "                                for attempt in range(retry_limit):\n",
    "                                    try:\n",
    "                                        # Rotate proxies\n",
    "                                        proxy = random.choice(PROXIES)\n",
    "                                        proxies = {'http': proxy, 'https': proxy} if proxy else None\n",
    "                                        \n",
    "                                        # Download the image\n",
    "                                        img_response = requests.get(\n",
    "                                            img_url, \n",
    "                                            headers=HEADERS, \n",
    "                                            proxies=proxies,\n",
    "                                            timeout=10\n",
    "                                        )\n",
    "                                        \n",
    "                                        if img_response.status_code == 200:\n",
    "                                            with open(img_path, 'wb') as f:\n",
    "                                                f.write(img_response.content)\n",
    "                                            article['images'].append({\n",
    "                                                'url': img_url,\n",
    "                                                'local_path': img_path\n",
    "                                            })\n",
    "                                            logger.info(f\"  Saved image: {img_path}\")\n",
    "                                            break\n",
    "                                    except Exception as e:\n",
    "                                        logger.warning(f\"  Image download attempt {attempt+1} failed: {str(e)}\")\n",
    "                                        if attempt == retry_limit - 1:\n",
    "                                            logger.error(f\"  Failed to download image {img_url} after {retry_limit} attempts\")\n",
    "                            except Exception as e:\n",
    "                                logger.error(f\"  Error processing image {img_url}: {e}\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"  Error processing article: {e}\")\n",
    "        \n",
    "        return headlines\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error scraping CNN: {e}\")\n",
    "        return []\n",
    "\n",
    "# Get the news with enhanced functionality\n",
    "print(\"CNN News Headlines with Content and Images:\")\n",
    "articles = get_cnn_news(save_images=True, fetch_content=True, max_articles=5)\n",
    "\n",
    "# Display the results\n",
    "for idx, article in enumerate(articles, 1):\n",
    "    print(f\"\\n{idx}. {article['headline']}\")\n",
    "    print(f\"   Link: {article['link']}\")\n",
    "    \n",
    "    # Show content preview (first 200 characters)\n",
    "    content_preview = article['content'][:200].replace('\\n', ' ')\n",
    "    if article['content'] and len(article['content']) > 200:\n",
    "        content_preview += \"...\"\n",
    "    print(f\"   Content preview: {content_preview}\")\n",
    "    \n",
    "    # Show images\n",
    "    if article['images']:\n",
    "        print(f\"   Downloaded {len(article['images'])} images\")\n",
    "    else:\n",
    "        print(\"   No images downloaded\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Save all data to a markdown file for easy viewing\n",
    "markdown_path = f\"cnn_news_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "with open(markdown_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"# CNN News Articles - {datetime.now().strftime('%Y-%m-%d')}\\n\\n\")\n",
    "    \n",
    "    for idx, article in enumerate(articles, 1):\n",
    "        f.write(f\"## {idx}. {article['headline']}\\n\\n\")\n",
    "        f.write(f\"**Link**: [{article['link']}]({article['link']})\\n\\n\")\n",
    "        \n",
    "        if article['content']:\n",
    "            f.write(\"### Content:\\n\\n\")\n",
    "            f.write(article['content'].replace('\\n', '\\n\\n'))\n",
    "            f.write(\"\\n\\n\")\n",
    "        \n",
    "        if article['images']:\n",
    "            f.write(\"### Images:\\n\\n\")\n",
    "            for img in article['images']:\n",
    "                f.write(f\"- [{img['url']}]({img['local_path']})\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"---\\n\\n\")\n",
    "\n",
    "print(f\"\\nSaved all articles to {markdown_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
